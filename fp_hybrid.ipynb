{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "550 Final Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQ5bgaFxiOEI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Conv1D, MaxPooling1D, LSTM, Dense, Dropout\n",
        "from keras.layers import TextVectorization\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import nltk\n",
        "from nltk.stem import \tWordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import preprocessor as p\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# nltk.download('punkt')\n",
        "# nltk.download('wordnet')\n",
        "# nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "JpT_S-EN5arH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "# !unzip -q glove.6B.zip"
      ],
      "metadata": {
        "id": "7Mguzkp_DorD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd 550\\ Final\\ Project"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBAegYEipqrl",
        "outputId": "221aee9e-4e85-4d9a-dacf-4765052e83dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '550 Final Project'\n",
            "/content/550 Final Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the datasets"
      ],
      "metadata": {
        "id": "p3qOhBJbpmDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(\"data/Constraint_Train.csv\")\n",
        "val = pd.read_csv(\"data/Constraint_Val.csv\")\n",
        "test = pd.read_csv(\"data/english_test_with_labels.csv\")\n",
        "train_c, train_l = train['tweet'].to_numpy(), train['label'].to_numpy()\n",
        "train_l = np.array([0 if i == 'fake' else 1 for i in train_l])\n",
        "val_c, val_l = val['tweet'].to_numpy(), val['label'].to_numpy()\n",
        "val_l = np.array([0 if i == 'fake' else 1 for i in val_l])\n",
        "test_c, test_l = test['tweet'].to_numpy(), test['label'].to_numpy()\n",
        "test_l = np.array([0 if i == 'fake' else 1 for i in test_l])\n",
        "# print(train_c)\n",
        "# display(train.head())"
      ],
      "metadata": {
        "id": "x8n3A8EKpkrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess"
      ],
      "metadata": {
        "id": "pL6aveYz744E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "i3EJj8zVJvoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "porter_stemmer  = PorterStemmer()\n",
        "\n",
        "p.set_options(p.OPT.URL, p.OPT.EMOJI)\n",
        "\n",
        "def preprocess(row, lemmatizer, stemmer):\n",
        "    txt = row\n",
        "    txt = p.clean(txt)\n",
        "    tokenization = nltk.word_tokenize(txt)     \n",
        "    tokenization = [w for w in tokenization if not w in stop_words]\n",
        "    # txt = ' '.join([porter_stemmer.stem(w) for w in tokenization])\n",
        "    # txt = ' '.join([lemmatizer.lemmatize(w) for w in txt])\n",
        "    txt = re.sub(r'[^a-zA-Z ]', '', txt).lower().strip()    \n",
        "    return txt\n",
        "\n",
        "train_c = [preprocess(x, wordnet_lemmatizer, porter_stemmer) for x in train_c]\n",
        "val_c = [preprocess(x, wordnet_lemmatizer, porter_stemmer) for x in val_c]\n",
        "test_c = [preprocess(x, wordnet_lemmatizer, porter_stemmer) for x in test_c]\n",
        "\n"
      ],
      "metadata": {
        "id": "CJjylA2n733O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "EP1SW0DXJsq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the model will remember only the top 20000 most common words\n",
        "max_words = 20000\n",
        "max_len = 300\n",
        "voc = np.array(train_c + val_c + test_c)\n",
        "token = Tokenizer(num_words=max_words, lower=True, split=' ')\n",
        "token.fit_on_texts(voc)\n",
        "sequences = token.texts_to_sequences(train_c)\n",
        "train_sequences_padded = pad_sequences(sequences, maxlen=max_len)\n",
        "\n",
        "sequences = token.texts_to_sequences(val_c)\n",
        "val_sequences_padded = pad_sequences(sequences, maxlen=max_len)\n",
        "\n",
        "sequences = token.texts_to_sequences(test_c)\n",
        "test_sequences_padded = pad_sequences(sequences, maxlen=max_len)"
      ],
      "metadata": {
        "id": "S5bAwFKea44B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def net():\n",
        "    model = Sequential([\n",
        "        Embedding(max_words, 100, input_length=300),\n",
        "\n",
        "        Conv1D(32, 8, activation='relu', padding=\"same\"),\n",
        "        MaxPooling1D(2),\n",
        "\n",
        "        LSTM(32),\n",
        "        Dense(10, activation=\"relu\"),\n",
        "        Dropout(0.5),\n",
        "        Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
        "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                  optimizer=opt, metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "EpRS9c05Sv7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = net()\n",
        "model.fit(train_sequences_padded, train_l, batch_size=32, epochs=5,\n",
        "          validation_data=(val_sequences_padded, val_l))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HoK6Z8qQqgO",
        "outputId": "7c714dc8-7d58-469a-9ee6-82abc6412129"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "201/201 [==============================] - 5s 14ms/step - loss: 0.5027 - accuracy: 0.7671 - val_loss: 0.2807 - val_accuracy: 0.8953\n",
            "Epoch 2/5\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.2284 - accuracy: 0.9198 - val_loss: 0.2172 - val_accuracy: 0.9178\n",
            "Epoch 3/5\n",
            "201/201 [==============================] - 2s 11ms/step - loss: 0.1204 - accuracy: 0.9625 - val_loss: 0.2192 - val_accuracy: 0.9215\n",
            "Epoch 4/5\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.0717 - accuracy: 0.9808 - val_loss: 0.2588 - val_accuracy: 0.9243\n",
            "Epoch 5/5\n",
            "201/201 [==============================] - 2s 12ms/step - loss: 0.0496 - accuracy: 0.9829 - val_loss: 0.3309 - val_accuracy: 0.9262\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3dc6f0e790>"
            ]
          },
          "metadata": {},
          "execution_count": 646
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model.predict(test_sequences_padded)\n",
        "pred = pred.reshape(2140)\n",
        "pred = np.array([0 if i < 0.5 else 1 for i in pred])"
      ],
      "metadata": {
        "id": "inENrYlzgv7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = accuracy_score(test_l, pred)\n",
        "print(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whnyAK2hhhVV",
        "outputId": "916214dd-4836-4eab-894e-3e76b8c8e991"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.935981308411215\n"
          ]
        }
      ]
    }
  ]
}